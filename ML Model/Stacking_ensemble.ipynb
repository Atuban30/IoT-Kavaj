{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef6d7b1-0761-4596-9ef7-d6d9e32a4ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (is_attack):\n",
      "is_attack\n",
      "1    0.795396\n",
      "0    0.204604\n",
      "Name: proportion, dtype: float64\n",
      "Stacking Ensemble Performance:\n",
      "Accuracy: 0.9983\n",
      "Precision: 0.9988\n",
      "Recall: 0.9991\n",
      "F1-Score: 0.9990\n",
      "\n",
      "Confusion Matrix Details:\n",
      "True Negatives (Non-Attack, Predicted Non-Attack): 855\n",
      "False Positives (Non-Attack, Predicted Attack): 4\n",
      "False Negatives (Attack, Predicted Non-Attack): 3\n",
      "True Positives (Attack, Predicted Attack): 3335\n",
      "Model components saved as .pkl files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from collections import Counter\n",
    "import math\n",
    "try:\n",
    "    from scapy.all import sniff, IP, TCP, UDP, get_if_list\n",
    "except ImportError as e:\n",
    "    print(f'Scapy import error: {e}. Ensure Scapy is installed (`pip install scapy`) and no file is named \\'scapy.py\\'.')\n",
    "    exit(1)\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print('Class Distribution (is_attack):')\n",
    "    print(df['is_attack'].value_counts(normalize=True))\n",
    "    features = ['time_delta', 'pps', 'length', 'src_ip_entropy', 'syn_flag', 'ack_flag']\n",
    "    X = df[features]\n",
    "    y = df['is_attack']\n",
    "    X = X.fillna(X.mean())\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, features, scaler\n",
    "\n",
    "def train_base_models(X_train, y_train, X_test):\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    rf_pred_train = rf_model.predict_proba(X_train)[:, 1]\n",
    "    xgb_pred_train = xgb_model.predict_proba(X_train)[:, 1]\n",
    "    rf_pred_test = rf_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_pred_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    meta_features_train = np.column_stack((rf_pred_train, xgb_pred_train))\n",
    "    meta_features_test = np.column_stack((rf_pred_test, xgb_pred_test))\n",
    "    return meta_features_train, meta_features_test, rf_model, xgb_model\n",
    "\n",
    "def train_stacking_ensemble(meta_features_train, y_train, meta_features_test):\n",
    "    meta_learner = LogisticRegression(random_state=42)\n",
    "    meta_learner.fit(meta_features_train, y_train)\n",
    "    final_predictions = meta_learner.predict(meta_features_test)\n",
    "    return final_predictions, meta_learner\n",
    "\n",
    "def evaluate_model(y_test, predictions):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, zero_division=0)\n",
    "    recall = recall_score(y_test, predictions, zero_division=0)\n",
    "    f1 = f1_score(y_test, predictions, zero_division=0)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    return cm\n",
    "\n",
    "def plot_feature_importance(rf_model, features):\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title('Feature Importance (Random Forest)')\n",
    "    plt.bar(range(len(features)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(features)), [features[i] for i in indices], rotation=45)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "def calculate_entropy(ip_list):\n",
    "    counts = Counter(ip_list)\n",
    "    total = sum(counts.values())\n",
    "    entropy = -sum((count/total) * math.log2(count/total) for count in counts.values()) if total > 0 else 0\n",
    "    return entropy\n",
    "\n",
    "def predict_packet(features, scaler, rf_model, xgb_model, meta_learner):\n",
    "    features_scaled = scaler.transform([features])\n",
    "    rf_pred = rf_model.predict_proba(features_scaled)[:, 1]\n",
    "    xgb_pred = xgb_model.predict_proba(features_scaled)[:, 1]\n",
    "    meta_features = np.column_stack((rf_pred, xgb_pred))\n",
    "    prediction = meta_learner.predict(meta_features)[0]\n",
    "    return prediction\n",
    "\n",
    "def realtime_detection(interface='Wi-Fi', target_ip='192.168.0.110', window_size=10, output_log='detection_log.txt'):\n",
    "    try:\n",
    "        meta_learner = joblib.load('stacking_model.pkl')\n",
    "        rf_model = joblib.load('rf_model.pkl')\n",
    "        xgb_model = joblib.load('xgb_model.pkl')\n",
    "        scaler = joblib.load('scaler.pkl')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Model file not found: {e}. Run training first to generate .pkl files.')\n",
    "        exit(1)\n",
    "    \n",
    "    available_interfaces = get_if_list()\n",
    "    if interface not in available_interfaces:\n",
    "        print(f'Interface \\'{interface}\\' not found. Available interfaces: {available_interfaces}')\n",
    "        exit(1)\n",
    "    \n",
    "    packets = []\n",
    "    log_file = open(output_log, 'w')\n",
    "    \n",
    "    def process_packet(packet):\n",
    "        if packet.haslayer(IP) and (packet[IP].src == target_ip or packet[IP].dst == target_ip):\n",
    "            pkt_time = time.time()\n",
    "            packets.append((pkt_time, packet))\n",
    "            packets[:] = [p for p in packets if pkt_time - window_size <= p[0]]\n",
    "            \n",
    "            if len(packets) >= 2:\n",
    "                pkt = packet\n",
    "                time_delta = pkt_time - packets[-2][0] if len(packets) > 1 else 0\n",
    "                pps = len(packets) / window_size\n",
    "                src_ips = [p[IP].src for t, p in packets]\n",
    "                src_ip_entropy = calculate_entropy(src_ips)\n",
    "                length = len(pkt)\n",
    "                syn_flag = 1 if pkt.haslayer(TCP) and pkt[TCP].flags & 0x02 else 0\n",
    "                ack_flag = 1 if pkt.haslayer(TCP) and pkt[TCP].flags & 0x10 else 0\n",
    "                \n",
    "                features = [time_delta, pps, length, src_ip_entropy, syn_flag, ack_flag]\n",
    "                \n",
    "                is_attack = predict_packet(features, scaler, rf_model, xgb_model, meta_learner)\n",
    "                \n",
    "                result = f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {'Attack Detected!' if is_attack else 'No Attack'} \" \\\n",
    "                         f\"(pps={pps:.2f}, time_delta={time_delta:.4f}, syn_flag={syn_flag})\"\n",
    "                print(result)\n",
    "                log_file.write(result + '\\n')\n",
    "                log_file.flush()\n",
    "    \n",
    "    print(f'Starting real-time DDoS detection on {interface} targeting {target_ip}...')\n",
    "    try:\n",
    "        sniff(iface=interface, prn=process_packet, store=False)\n",
    "    except PermissionError:\n",
    "        print('Permission denied. Run the script as administrator or install npcap (https://nmap.org/npcap/).')\n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopping detection...')\n",
    "    finally:\n",
    "        log_file.close()\n",
    "\n",
    "def main():\n",
    "    file_path = r'C:\\Users\\DELL\\Desktop\\Final_year_Project\\Datas\\DDoS\\Correct\\One-attack, labeled.csv'\n",
    "    X, y, features, scaler = load_and_preprocess_data(file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    meta_features_train, meta_features_test, rf_model, xgb_model = train_base_models(X_train, y_train, X_test)\n",
    "    final_predictions, meta_learner = train_stacking_ensemble(meta_features_train, y_train, meta_features_test)\n",
    "    print('Stacking Ensemble Performance:')\n",
    "    cm = evaluate_model(y_test, final_predictions)\n",
    "    print('\\nConfusion Matrix Details:')\n",
    "    print(f'True Negatives (Non-Attack, Predicted Non-Attack): {cm[0,0]}')\n",
    "    print(f'False Positives (Non-Attack, Predicted Attack): {cm[0,1]}')\n",
    "    print(f'False Negatives (Attack, Predicted Non-Attack): {cm[1,0]}')\n",
    "    print(f'True Positives (Attack, Predicted Attack): {cm[1,1]}')\n",
    "    plot_feature_importance(rf_model, features)\n",
    "    \n",
    "    joblib.dump(meta_learner, 'stacking_model.pkl')\n",
    "    joblib.dump(rf_model, 'rf_model.pkl')\n",
    "    joblib.dump(xgb_model, 'xgb_model.pkl')\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    print('Model components saved as .pkl files.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == 'realtime':\n",
    "        realtime_detection()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba687bae-e68a-4f31-9625-551084d8d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "import math\n",
    "import joblib\n",
    "from scapy.all import sniff, IP, TCP, UDP, get_if_list\n",
    "\n",
    "# Load trained model components\n",
    "try:\n",
    "    meta_learner = joblib.load('stacking_model.pkl')\n",
    "    rf_model = joblib.load('rf_model.pkl')\n",
    "    xgb_model = joblib.load('xgb_model.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "except FileNotFoundError as e:\n",
    "    print(f'Error: {e}. Ensure .pkl files are in the working directory.')\n",
    "    raise\n",
    "\n",
    "def calculate_entropy(ip_list):\n",
    "    counts = Counter(ip_list)\n",
    "    total = sum(counts.values())\n",
    "    entropy = -sum((count/total) * math.log2(count/total) for count in counts.values()) if total > 0 else 0\n",
    "    return entropy\n",
    "\n",
    "def predict_packet(features_df):\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "    rf_pred = rf_model.predict_proba(features_scaled)[:, 1]\n",
    "    xgb_pred = xgb_model.predict_proba(features_scaled)[:, 1]\n",
    "    meta_features = np.column_stack((rf_pred, xgb_pred))\n",
    "    return meta_learner.predict(meta_features)[0]\n",
    "\n",
    "def realtime_detection(interface='your-interface', target_ip='ESP32-ip address', window_size=10):\n",
    "    packets = []\n",
    "    feature_names = ['time_delta', 'pps', 'length', 'src_ip_entropy', 'syn_flag', 'ack_flag']\n",
    "    \n",
    "    def process_packet(packet):\n",
    "        if packet.haslayer(IP) and (packet[IP].src == target_ip or packet[IP].dst == target_ip):\n",
    "            pkt_time = time.time()\n",
    "            packets.append((pkt_time, packet))\n",
    "            packets[:] = [p for p in packets if pkt_time - window_size <= p[0]]\n",
    "            \n",
    "            if len(packets) >= 2:\n",
    "                pkt = packet\n",
    "                time_delta = pkt_time - packets[-2][0] if len(packets) > 1 else 0\n",
    "                pps = len(packets) / window_size\n",
    "                src_ips = [p[IP].src for t, p in packets]\n",
    "                src_ip_entropy = calculate_entropy(src_ips)\n",
    "                length = len(pkt)\n",
    "                syn_flag = 1 if pkt.haslayer(TCP) and pkt[TCP].flags & 0x02 else 0\n",
    "                ack_flag = 1 if pkt.haslayer(TCP) and pkt[TCP].flags & 0x10 else 0\n",
    "                \n",
    "                features = [[time_delta, pps, length, src_ip_entropy, syn_flag, ack_flag]]\n",
    "                features_df = pd.DataFrame(features, columns=feature_names)\n",
    "                \n",
    "                is_attack = predict_packet(features_df)\n",
    "                \n",
    "                result = f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {'Attack Detected!' if is_attack else 'No Attack'} \" \\\n",
    "                         f\"(pps={pps:.2f}, time_delta={time_delta:.4f}, syn_flag={syn_flag})\"\n",
    "                print(result)\n",
    "                \n",
    "                with open('detection_log.txt', 'a') as f:\n",
    "                    f.write(result + '\\n')\n",
    "    \n",
    "    print(f'Starting real-time detection on {interface} for {target_ip}...')\n",
    "    try:\n",
    "        sniff(iface=interface, prn=process_packet, store=False)\n",
    "    except PermissionError:\n",
    "        print('Permission denied. Run PyCharm as administrator and ensure npcap is installed (https://nmap.org/npcap/).')\n",
    "    except Exception as e:\n",
    "        print(f'Error during packet capture: {e}')\n",
    "    finally:\n",
    "        print('Detection stopped.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    realtime_detection(interface='your-interface', target_ip='ESP32-ip address', window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9250d64-ce1c-4840-aa39-5e821c485215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
